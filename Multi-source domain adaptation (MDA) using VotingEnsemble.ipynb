{"cells":[{"cell_type":"markdown","metadata":{"id":"qNbm6TmM7370"},"source":["**Introduction**"]},{"cell_type":"markdown","metadata":{"id":"mUXMczjc8Gxe"},"source":["Multi-source domain adaptation (MDA) is a fascinating area in machine learning that aims to transfer knowledge from multiple source domains to an unlabeled target domain.\n","\n","In MDA, the goal is to leverage the information from all available source domains to improve the performance of the model on the target domain."]},{"cell_type":"markdown","metadata":{"id":"tJU4y5dovXka"},"source":["Ensemble"]},{"cell_type":"markdown","metadata":{"id":"uLDCRgWdvbGC"},"source":["Ensemble methods work because they combine the strengths of multiple models to produce a more robust and accurate prediction or decision than any single model could achieve on its own"]},{"cell_type":"markdown","metadata":{"id":"6HRS94Wwvb-Z"},"source":["Source-Specific Models"]},{"cell_type":"markdown","metadata":{"id":"bVpf2vwPvdm8"},"source":["Source-Specific Models: Train individual models for each source domain and combine their predictions for the target domain using ensemble techniques like weighted averaging or majority voting"]},{"cell_type":"markdown","metadata":{"id":"c8iicU2s-Kba"},"source":["Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"executionInfo":{"elapsed":9,"status":"ok","timestamp":1741408365568,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":360},"id":"J1O_bU30Mfvj"},"outputs":[],"source":["# !pip install torchensemble"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2819,"status":"ok","timestamp":1741408368396,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":360},"id":"nUjZ0mgl-XOm"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","\n","from torchensemble import VotingClassifier\n","from torchensemble.utils.logging import set_logger"]},{"cell_type":"markdown","metadata":{"id":"p8k-SMky_dyY"},"source":["Data Processing"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"apsfc3-H_f_s","executionInfo":{"status":"ok","timestamp":1741408370300,"user_tz":360,"elapsed":1892,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"af86a904-720b-4b65-ea42-53f47b214016"},"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/mei1963/domainnet/versions/1\n"]}],"source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"mei1963/domainnet\")\n","\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","source":["import os\n","def walk_through_dir(dir_path):\n","  \"\"\"\n","  Walks through dir_path returning its contents.\n","  Args:\n","    dir_path (str or pathlib.Path): target directory\n","\n","  Returns:\n","    A print out of:\n","      number of subdiretories in dir_path\n","      number of images (files) in each subdirectory\n","      name of each subdirectory\n","  \"\"\"\n","  for dirpath, dirnames, filenames in os.walk(dir_path):\n","    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n","\n","# walk_through_dir(path)"],"metadata":{"id":"zYy8cdOD8LP9","executionInfo":{"status":"ok","timestamp":1741408370304,"user_tz":360,"elapsed":2,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"dete1ThQI5qm","executionInfo":{"status":"ok","timestamp":1741408370307,"user_tz":360,"elapsed":2,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.ToTensor(),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])"]},{"cell_type":"code","source":["simple_transform = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.ToTensor(),\n","\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])"],"metadata":{"id":"RqkO1ZqfAEs2","executionInfo":{"status":"ok","timestamp":1741408370323,"user_tz":360,"elapsed":2,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"QV9emEKwJzaj","executionInfo":{"status":"ok","timestamp":1741408370337,"user_tz":360,"elapsed":13,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["real = os.path.join(path, \"DomainNet/real\")\n","sketch = os.path.join(path, \"DomainNet/sketch\")\n","clip_art = os.path.join(path, \"DomainNet/clipart\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"P0KUpuK3KXY_","executionInfo":{"status":"ok","timestamp":1741408370978,"user_tz":360,"elapsed":639,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["real_dataset = ImageFolder(real, transform=transform)\n","sketch_dataset = ImageFolder(sketch, transform=transform)\n","clipart_dataset = ImageFolder(clip_art, transform=transform)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"1HOo2DttLY2U","executionInfo":{"status":"ok","timestamp":1741408370990,"user_tz":360,"elapsed":2,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["real_dataloader = DataLoader(real_dataset, batch_size=32, shuffle=True)\n","sketch_dataloader = DataLoader(sketch_dataset, batch_size=32, shuffle=True)\n","clipart_dataloader = DataLoader(clipart_dataset, batch_size=32, shuffle=True)"]},{"cell_type":"code","source":["len(real_dataset), len(sketch_dataset), len(clipart_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZzdCR1FROA4","executionInfo":{"status":"ok","timestamp":1741408370995,"user_tz":360,"elapsed":4,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"f2501f6c-870d-4842-ae60-485bbb0036ec"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(175327, 70386, 48833)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"2DoqPs7w_cFs"},"source":["Models"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"t6_ry9qz-2TL","executionInfo":{"status":"ok","timestamp":1741408911467,"user_tz":360,"elapsed":4,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["class SimpleCNN(nn.Module):\n","    def __init__(self, num_classes=345):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n","        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n","        self.fc2 = nn.Linear(512, num_classes)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"RKSp6RK4MOZR"},"source":["Initialize the Ensemble"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"rnmxIL8QMM4l","executionInfo":{"status":"ok","timestamp":1741408912667,"user_tz":360,"elapsed":11,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["ensemble = VotingClassifier(\n","    estimator = SimpleCNN,\n","    n_estimators = 3,\n","    cuda = torch.cuda.is_available(),\n","    voting_strategy= \"hard\",\n","    estimator_args={\"num_classes\": 345}\n",")"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"V5-2K-KmNpL_","executionInfo":{"status":"ok","timestamp":1741408913976,"user_tz":360,"elapsed":1,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["# Set the loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","ensemble.set_criterion(criterion)\n","ensemble.set_optimizer(\"Adam\", lr=0.0001)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"OFM54g5GNtJu","executionInfo":{"status":"ok","timestamp":1741408914745,"user_tz":360,"elapsed":1,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["# List of DataLoaders for different domains\n","dataloaders = [real_dataloader, sketch_dataloader, clipart_dataloader]"]},{"cell_type":"code","source":["img, label = next(iter(clipart_dataloader))\n","\n","# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n","print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n","print(f\"Label shape: {label.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dVN-3Q5tCbHk","executionInfo":{"status":"ok","timestamp":1741408915476,"user_tz":360,"elapsed":112,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"61130665-6423-452f-f1d1-0a9aae19ace9"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Image shape: torch.Size([32, 3, 32, 32]) -> [batch_size, color_channels, height, width]\n","Label shape: torch.Size([32])\n"]}]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QW9zVIrcQ85G","executionInfo":{"status":"ok","timestamp":1741408917246,"user_tz":360,"elapsed":15,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"2d25732f-f70f-496f-a4ce-ccdad5806d59"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<RootLogger root (DEBUG)>"]},"metadata":{},"execution_count":43}],"source":["# Set the logger to print out training details\n","set_logger()"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"Rx9B1ixOSLRP","executionInfo":{"status":"ok","timestamp":1741408918020,"user_tz":360,"elapsed":6,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"82c84ada-b727-4580-c876-ba9f700be5eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method VotingClassifier.fit of VotingClassifier(\n","  (estimators_): ModuleList()\n","  (_criterion): CrossEntropyLoss()\n",")>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>torchensemble.voting.VotingClassifier.fit</b><br/>def fit(train_loader, epochs=100, log_interval=100, test_loader=None, save_model=True, save_dir=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/torchensemble/voting.py</a>Implementation on the training stage of VotingClassifier.\n","\n","\n","Parameters\n","----------\n","train_loader : torch.utils.data.DataLoader\n","    A :mod:`torch.utils.data.DataLoader` container that contains the\n","    training data.\n","epochs : int, default=100\n","    The number of training epochs.\n","log_interval : int, default=100\n","    The number of batches to wait before logging the training status.\n","test_loader : torch.utils.data.DataLoader, default=None\n","    A :mod:`torch.utils.data.DataLoader` container that contains the\n","    evaluating data.\n","\n","    - If ``None``, no validation is conducted during the training\n","      stage.\n","    - If not ``None``, the ensemble will be evaluated on this\n","      dataloader after each training epoch.\n","save_model : bool, default=True\n","    Specify whether to save the model parameters.\n","\n","    - If test_loader is ``None``, the ensemble fully trained will be\n","      saved.\n","    - If test_loader is not ``None``, the ensemble with the best\n","      validation performance will be saved.\n","save_dir : string, default=None\n","    Specify where to save the model parameters.\n","\n","    - If ``None``, the model will be saved in the current directory.\n","    - If not ``None``, the model will be saved in the specified\n","      directory: ``save_dir``.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 149);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":44}],"source":["ensemble.fit"]},{"cell_type":"markdown","source":["Train on Multiple Source Domains Sequentially"],"metadata":{"id":"Qsa84adGPFz3"}},{"cell_type":"code","source":["def train_on_multiple_domains(ensemble, dataloaders, epochs=5):\n","    for domain_idx, dataloader in enumerate(dataloaders):\n","        print(f\"Training on Domain {domain_idx}...\")\n","        ensemble.fit(dataloader, epochs=epochs, log_interval=1000)\n","        print(f\"Finished training on Domain {domain_idx}.\")\n","\n","\n","\n","# Train on multiple source domains\n","train_on_multiple_domains(ensemble, dataloaders, epochs=3)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dz0is-TTof6","outputId":"f363ed5c-1c17-4aba-be18-8b3cb538d383"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on Domain 0...\n","Estimator: 000 | Epoch: 000 | Batch: 000 | Loss: 5.84377 | Correct: 0/32\n","Estimator: 000 | Epoch: 000 | Batch: 1000 | Loss: 4.93720 | Correct: 1/32\n","Estimator: 000 | Epoch: 000 | Batch: 2000 | Loss: 4.41009 | Correct: 4/32\n","Estimator: 000 | Epoch: 000 | Batch: 3000 | Loss: 4.16343 | Correct: 4/32\n","Estimator: 000 | Epoch: 000 | Batch: 4000 | Loss: 5.06649 | Correct: 1/32\n","Estimator: 000 | Epoch: 000 | Batch: 5000 | Loss: 4.40790 | Correct: 3/32\n","Estimator: 001 | Epoch: 000 | Batch: 000 | Loss: 5.85139 | Correct: 0/32\n","Estimator: 001 | Epoch: 000 | Batch: 1000 | Loss: 4.52578 | Correct: 4/32\n","Estimator: 001 | Epoch: 000 | Batch: 2000 | Loss: 4.59016 | Correct: 4/32\n","Estimator: 001 | Epoch: 000 | Batch: 3000 | Loss: 4.16539 | Correct: 6/32\n","Estimator: 001 | Epoch: 000 | Batch: 4000 | Loss: 4.06004 | Correct: 5/32\n","Estimator: 001 | Epoch: 000 | Batch: 5000 | Loss: 3.78441 | Correct: 4/32\n","Estimator: 002 | Epoch: 000 | Batch: 000 | Loss: 5.84166 | Correct: 0/32\n","Estimator: 002 | Epoch: 000 | Batch: 1000 | Loss: 4.92955 | Correct: 2/32\n","Estimator: 002 | Epoch: 000 | Batch: 2000 | Loss: 4.13837 | Correct: 4/32\n","Estimator: 002 | Epoch: 000 | Batch: 3000 | Loss: 4.62697 | Correct: 3/32\n","Estimator: 002 | Epoch: 000 | Batch: 4000 | Loss: 4.45517 | Correct: 3/32\n","Estimator: 002 | Epoch: 000 | Batch: 5000 | Loss: 3.96310 | Correct: 7/32\n"]},{"output_type":"stream","name":"stderr","text":["2025-03-08 05:10:40,744 - INFO: Saving the model to `./VotingClassifier_SimpleCNN_3_ckpt.pth`\n"]},{"output_type":"stream","name":"stdout","text":["Estimator: 000 | Epoch: 001 | Batch: 000 | Loss: 3.66082 | Correct: 7/32\n","Estimator: 000 | Epoch: 001 | Batch: 1000 | Loss: 4.11870 | Correct: 4/32\n","Estimator: 000 | Epoch: 001 | Batch: 2000 | Loss: 3.63262 | Correct: 7/32\n","Estimator: 000 | Epoch: 001 | Batch: 3000 | Loss: 3.54670 | Correct: 5/32\n","Estimator: 000 | Epoch: 001 | Batch: 4000 | Loss: 3.89510 | Correct: 4/32\n","Estimator: 000 | Epoch: 001 | Batch: 5000 | Loss: 3.12076 | Correct: 12/32\n","Estimator: 001 | Epoch: 001 | Batch: 000 | Loss: 3.51839 | Correct: 5/32\n","Estimator: 001 | Epoch: 001 | Batch: 1000 | Loss: 3.88210 | Correct: 4/32\n","Estimator: 001 | Epoch: 001 | Batch: 2000 | Loss: 3.72030 | Correct: 8/32\n","Estimator: 001 | Epoch: 001 | Batch: 3000 | Loss: 3.39476 | Correct: 6/32\n","Estimator: 001 | Epoch: 001 | Batch: 4000 | Loss: 4.10638 | Correct: 2/32\n","Estimator: 001 | Epoch: 001 | Batch: 5000 | Loss: 3.81811 | Correct: 6/32\n","Estimator: 002 | Epoch: 001 | Batch: 000 | Loss: 3.83399 | Correct: 7/32\n","Estimator: 002 | Epoch: 001 | Batch: 1000 | Loss: 3.57064 | Correct: 7/32\n","Estimator: 002 | Epoch: 001 | Batch: 2000 | Loss: 3.95578 | Correct: 5/32\n","Estimator: 002 | Epoch: 001 | Batch: 3000 | Loss: 3.53007 | Correct: 8/32\n","Estimator: 002 | Epoch: 001 | Batch: 4000 | Loss: 3.87677 | Correct: 6/32\n","Estimator: 002 | Epoch: 001 | Batch: 5000 | Loss: 4.02523 | Correct: 9/32\n"]},{"output_type":"stream","name":"stderr","text":["2025-03-08 05:39:59,768 - INFO: Saving the model to `./VotingClassifier_SimpleCNN_3_ckpt.pth`\n"]},{"output_type":"stream","name":"stdout","text":["Estimator: 000 | Epoch: 002 | Batch: 000 | Loss: 3.40195 | Correct: 12/32\n","Estimator: 000 | Epoch: 002 | Batch: 1000 | Loss: 3.07100 | Correct: 8/32\n","Estimator: 000 | Epoch: 002 | Batch: 2000 | Loss: 3.83516 | Correct: 2/32\n","Estimator: 000 | Epoch: 002 | Batch: 3000 | Loss: 3.74249 | Correct: 5/32\n","Estimator: 000 | Epoch: 002 | Batch: 4000 | Loss: 2.41340 | Correct: 12/32\n","Estimator: 000 | Epoch: 002 | Batch: 5000 | Loss: 3.36919 | Correct: 7/32\n","Estimator: 001 | Epoch: 002 | Batch: 000 | Loss: 3.41971 | Correct: 6/32\n","Estimator: 001 | Epoch: 002 | Batch: 1000 | Loss: 3.15194 | Correct: 5/32\n"]}]},{"cell_type":"code","source":["# print(f\"Number of estimators: {len(ensemble.estimators_)}\")\n"],"metadata":{"id":"kkFTrj8oVdWZ","executionInfo":{"status":"aborted","timestamp":1741408371714,"user_tz":360,"elapsed":24,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"v2iYMUG7c1pY","executionInfo":{"status":"aborted","timestamp":1741408371716,"user_tz":360,"elapsed":25,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the path where you want to save the model\n","save_path = \"/content/drive/MyDrive/ensemble/model.pth\"  # You can change this to your desired path\n","\n","\n","\n","# Ensure the directory exists\n","os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","\n","# Save the entire ensemble model\n","torch.save(ensemble, save_path)\n"],"metadata":{"id":"SH4pgamua3_L","executionInfo":{"status":"aborted","timestamp":1741408371717,"user_tz":360,"elapsed":26,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test Loop"],"metadata":{"id":"HxvxrR3DtHpP"}},{"cell_type":"code","source":["# Test path\n","quickdraw = os.path.join(path, \"DomainNet/quickdraw\")\n","\n","# Test dataset\n","quickdraw_dataset = ImageFolder(quickdraw, transform=transform)\n","\n","# Test dataloader\n","quickdraw_dataloader = DataLoader(quickdraw_dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"f3Z-tOMWtLdu","executionInfo":{"status":"aborted","timestamp":1741408371718,"user_tz":360,"elapsed":26,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model on the 4th dataset\n","def evaluate_on_new_data(ensemble, dataloader):\n","    ensemble.eval()  # Set the model to evaluation mode\n","\n","    correct = 0\n","    total = 0\n","    running_loss = 0.0\n","\n","    with torch.no_grad():  # Disable gradient computation for evaluation\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(ensemble.device), labels.to(ensemble.device)\n","\n","            outputs = ensemble(inputs)\n","            loss = ensemble._criterion(outputs, labels)\n","            running_loss += loss.item()\n","\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    avg_loss = running_loss / len(dataloader)\n","\n","    print(f\"Loss on new dataset: {avg_loss:.3f}\")\n","    print(f\"Accuracy on new dataset: {accuracy:.2f}%\")\n","\n","# Evaluate on the 4th dataset\n","evaluate_on_new_data(ensemble, quickdraw_dataloader)"],"metadata":{"id":"e8fIjhah2z9n","executionInfo":{"status":"aborted","timestamp":1741408371719,"user_tz":360,"elapsed":27,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Conclusion**"],"metadata":{"id":"fB_pgvH8yy7C"}},{"cell_type":"markdown","source":["It looks like we're getting low accuracy when testing our 4th dataset (QuickDraw), and underfitting is occurring during the training loop.Here are a few options we can implement next to increase our results."],"metadata":{"id":"JuIyaRHizTlU"}},{"cell_type":"markdown","source":["1. Increase Model Complexity\n","Enhance the capability of your model to capture complex patterns by increasing its depth and the number of filters.\n","\n","2. Data Augmentation\n","Use more diverse data augmentation techniques to enrich the training data.\n","\n","3. Fine-Tune on Specific Data\n","Fine-tune your ensemble on a portion of the QuickDraw dataset to help it adapt better.\n","\n","4. Hyperparameter Tuning\n","Experiment with different learning rates, batch sizes, and optimizers to find the optimal configuration.\n","\n","5. Regularization Techniques\n","Add dropout or L2 regularization to prevent overfitting and improve model robustness."],"metadata":{"id":"JLk8fN_10E8s"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"mount_file_id":"1XyyV6qzwVV76GC_7p7q0_R-Bg7pigsr3","authorship_tag":"ABX9TyN1+2sWeJzeJ5825kcid5ec"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}